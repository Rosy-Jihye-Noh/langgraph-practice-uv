{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb10d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e9b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os, json\n",
    "\n",
    "from textwrap import dedent\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7948c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c32cb26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence, Dict, Any\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import Graph, StateGraph\n",
    "from langchain.tools import tool\n",
    "import operator\n",
    "from typing import List, Dict, Any\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import streamlit as st\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c432d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒíƒœ ì •ì˜\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    memory: Dict[str, Any]\n",
    "    checkpoints: List[Dict]\n",
    "    next: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›¹ ìŠ¤í¬ë˜í•‘ ë„êµ¬\n",
    "@tool\n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"ì¸í„°ë„·ì—ì„œ ìì„¸ êµì • ê´€ë ¨ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤\"\"\"\n",
    "    try:\n",
    "        results = ddg(query, max_results=3)\n",
    "        return \"\\n\".join([f\"ì œëª©: {r['title']}\\në‚´ìš©: {r['body']}\\në§í¬: {r['link']}\\n\" for r in results])\n",
    "    except Exception as e:\n",
    "        return f\"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def scrape_posture_info(url: str) -> str:\n",
    "    \"\"\"íŠ¹ì • ì›¹ì‚¬ì´íŠ¸ì˜ ë‚´ìš©ì„ ìŠ¤í¬ë©í•˜ì—¬ Chroma DBì— ì €ì¥í•˜ê³  ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤\"\"\"\n",
    "    try:\n",
    "        # ì›¹ ìŠ¤í¬ë˜í•‘\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        title = soup.title.string if soup.title else \"ì œëª© ì—†ìŒ\"\n",
    "        content = soup.find('article') or soup.find('main') or soup.find('body')\n",
    "        \n",
    "        # ì´ë¯¸ì§€ URL ì¶”ì¶œ\n",
    "        images = []\n",
    "        if content:\n",
    "            for img in content.find_all('img'):\n",
    "                src = img.get('src', '')\n",
    "                alt = img.get('alt', 'ì´ë¯¸ì§€ ì„¤ëª… ì—†ìŒ')\n",
    "                if src and src.startswith(('http://', 'https://')):\n",
    "                    images.append({'url': src, 'alt': alt})\n",
    "                elif src:  # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\n",
    "                    full_url = requests.compat.urljoin(url, src)\n",
    "                    images.append({'url': full_url, 'alt': alt})\n",
    "\n",
    "        # ë™ì˜ìƒ URL ì¶”ì¶œ\n",
    "        videos = []\n",
    "        if content:\n",
    "            # YouTube iframes\n",
    "            for iframe in content.find_all('iframe'):\n",
    "                src = iframe.get('src', '')\n",
    "                if 'youtube' in src or 'vimeo' in src:\n",
    "                    videos.append({'url': src, 'type': 'embed'})\n",
    "            \n",
    "            # Video tags\n",
    "            for video in content.find_all('video'):\n",
    "                src = video.get('src', '')\n",
    "                if src:\n",
    "                    if not src.startswith(('http://', 'https://')):\n",
    "                        src = requests.compat.urljoin(url, src)\n",
    "                    videos.append({'url': src, 'type': 'video'})\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í¬ ë¶„í• \n",
    "        text = content.get_text(strip=True) if content else \"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        \n",
    "        # ê° ì²­í¬ì— ë¯¸ë””ì–´ ì •ë³´ í¬í•¨í•˜ì—¬ ì €ì¥\n",
    "        docs = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # ì²­í¬ë³„ë¡œ ê´€ë ¨ ë¯¸ë””ì–´ ë©”íƒ€ë°ì´í„° í¬í•¨\n",
    "            metadata = {\n",
    "                \"source\": url,\n",
    "                \"title\": title,\n",
    "                \"images\": images[i:i+2] if images else [],  # ê° ì²­í¬ë‹¹ ìµœëŒ€ 2ê°œ ì´ë¯¸ì§€\n",
    "                \"videos\": videos[i:i+1] if videos else []   # ê° ì²­í¬ë‹¹ ìµœëŒ€ 1ê°œ ë™ì˜ìƒ\n",
    "            }\n",
    "            docs.append(Document(page_content=chunk, metadata=metadata))\n",
    "\n",
    "        # Chroma DB ì¤‘ë³µ ì²´í¬ ì¶”ê°€\n",
    "        vector_store = Chroma(\n",
    "        collection_name=\"posture_info\",\n",
    "        embedding_function=embeddings_model,\n",
    "        persist_directory=\"./chroma_db\"\n",
    "        )\n",
    "    \n",
    "        # URL ê¸°ë°˜ ì¤‘ë³µ ì²´í¬\n",
    "        existing_docs = vector_store.get(\n",
    "        where={\"source\": url}\n",
    "        )\n",
    "    \n",
    "        if not existing_docs:\n",
    "            vector_store.add_documents(docs)\n",
    "        \n",
    "        # ê´€ë ¨ ì •ë³´ ê²€ìƒ‰\n",
    "        results = vector_store.similarity_search(\n",
    "            \"ìì„¸ êµì • ë°©ë²•ê³¼ íŒ\",\n",
    "            k=3\n",
    "        )\n",
    "        \n",
    "        # ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ… (ë¯¸ë””ì–´ í¬í•¨)\n",
    "        response = f\"ì›¹ì‚¬ì´íŠ¸ ì œëª©: {title}\\nURL: {url}\\n\\nê´€ë ¨ ì •ë³´:\\n\"\n",
    "        \n",
    "        for i, doc in enumerate(results, 1):\n",
    "            response += f\"\\n{i}. {doc.page_content[:300]}...\\n\"\n",
    "            \n",
    "            # ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€\n",
    "            if doc.metadata.get('images'):\n",
    "                response += \"\\nê´€ë ¨ ì´ë¯¸ì§€:\\n\"\n",
    "                for img in doc.metadata['images']:\n",
    "                    response += f\"![{img['alt']}]({img['url']})\\n\"\n",
    "            \n",
    "            # ë™ì˜ìƒ ì •ë³´ ì¶”ê°€\n",
    "            if doc.metadata.get('videos'):\n",
    "                response += \"\\nê´€ë ¨ ë™ì˜ìƒ:\\n\"\n",
    "                for video in doc.metadata['videos']:\n",
    "                    if video['type'] == 'embed':\n",
    "                        response += f\"ì„ë² ë“œ ë™ì˜ìƒ: {video['url']}\\n\"\n",
    "                    else:\n",
    "                        response += f\"ë¹„ë””ì˜¤ ë§í¬: {video['url']}\\n\"\n",
    "            \n",
    "            response += \"\\n---\\n\"\n",
    "            \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"ìŠ¤í¬ë˜í•‘ ë° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddfdb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct ì—ì´ì „íŠ¸ ì •ì˜ ìˆ˜ì •\n",
    "def create_agent():\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    tools = [search_web, scrape_posture_info]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"ë‹¹ì‹ ì€ ìì„¸ êµì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìì„¸ ë¬¸ì œë¥¼ ë¶„ì„í•˜ê³  ê°œì„  ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.\n",
    "        í•„ìš”í•œ ê²½ìš° ë‹¤ìŒ ë„êµ¬ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "        1. search_web: ìµœì‹  ìì„¸ êµì • ì •ë³´ ê²€ìƒ‰\n",
    "        2. scrape_posture_info: íŠ¹ì • URLì—ì„œ ìì„¸í•œ ì •ë³´ ì¶”ì¶œ\n",
    "        \n",
    "        ê²€ìƒ‰ì´ë‚˜ ìŠ¤í¬ë˜í•‘ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì¹œí™”ì ì¸ ì „ë¬¸ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.\"\"\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    return llm.bind(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac269ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬\n",
    "def save_checkpoint(state: AgentState) -> AgentState:\n",
    "    # í˜„ì¬ ì‹œê°„ ì¶”ê°€\n",
    "    from datetime import datetime\n",
    "    \n",
    "    checkpoint = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"conversation\": state[\"messages\"],\n",
    "        \"context\": state[\"memory\"]\n",
    "    }\n",
    "    state[\"checkpoints\"].append(checkpoint)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ êµ¬ì„±\n",
    "def create_graph():\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # ë…¸ë“œ ì¶”ê°€\n",
    "    workflow.add_node(\"agent\", create_agent())\n",
    "    workflow.add_node(\"checkpoint\", save_checkpoint)\n",
    "    \n",
    "    # ì—£ì§€ ì—°ê²°\n",
    "    workflow.add_edge(\"agent\", \"checkpoint\")\n",
    "    workflow.add_edge(\"checkpoint\", \"agent\")\n",
    "    \n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091dafb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#streamlit\n",
    "def create_streamlit_app():\n",
    "    st.set_page_config(\n",
    "        page_title=\"ìì„¸ êµì • AI íŠ¸ë ˆì´ë„ˆ\",\n",
    "        page_icon=\"ğŸ§˜â€â™€ï¸\",\n",
    "        layout=\"wide\"\n",
    "    )\n",
    "    \n",
    "    st.title(\"ìì„¸ êµì • AI íŠ¸ë ˆì´ë„ˆ ğŸ§˜â€â™€ï¸\")\n",
    "    st.markdown(\"ìì„¸ êµì • ê´€ë ¨ URLì„ ê³µìœ í•˜ì‹œë©´ í•´ë‹¹ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì¡°ì–¸í•´ë“œë¦½ë‹ˆë‹¤.\")\n",
    "\n",
    "    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "        st.session_state.graph = create_graph()\n",
    "\n",
    "    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            if message.get(\"images\") or message.get(\"videos\"):\n",
    "                # í…ìŠ¤íŠ¸ ë¨¼ì € í‘œì‹œ\n",
    "                st.markdown(message[\"content\"])\n",
    "                \n",
    "                # ì´ë¯¸ì§€ í‘œì‹œ\n",
    "                if message.get(\"images\"):\n",
    "                    cols = st.columns(len(message[\"images\"]))\n",
    "                    for idx, img in enumerate(message[\"images\"]):\n",
    "                        with cols[idx]:\n",
    "                            st.image(img[\"url\"], caption=img[\"alt\"])\n",
    "                \n",
    "                # ë¹„ë””ì˜¤ í‘œì‹œ\n",
    "                if message.get(\"videos\"):\n",
    "                    for video in message[\"videos\"]:\n",
    "                        if video[\"type\"] == \"embed\":\n",
    "                            st.video(video[\"url\"])\n",
    "                        else:\n",
    "                            st.markdown(f\"[ë¹„ë””ì˜¤ ë³´ê¸°]({video['url']})\")\n",
    "            else:\n",
    "                st.markdown(message[\"content\"])\n",
    "\n",
    "    # ì‚¬ìš©ì ì…ë ¥\n",
    "    if prompt := st.chat_input(\"ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”\"):\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(prompt)\n",
    "\n",
    "        # URL í™•ì¸ ë° ì²˜ë¦¬\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            with st.spinner(\"ë‹µë³€ ìƒì„± ì¤‘...\"):\n",
    "                state = {\n",
    "                    \"messages\": [HumanMessage(content=prompt)],\n",
    "                    \"memory\": {},\n",
    "                    \"checkpoints\": [],\n",
    "                    \"next\": \"agent\"\n",
    "                }\n",
    "                \n",
    "                url_pattern = r'https?://[^\\s]+'\n",
    "                urls = re.findall(url_pattern, prompt)\n",
    "                \n",
    "                if urls:\n",
    "                    for url in urls:\n",
    "                        info = scrape_posture_info(url)\n",
    "                        # ìŠ¤í¬ë© ì •ë³´ì—ì„œ ë¯¸ë””ì–´ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "                        media_info = extract_media_from_response(info)\n",
    "                        response = process_response(info, media_info)\n",
    "                else:\n",
    "                    result = st.session_state.graph.invoke(state)\n",
    "                    response = {\n",
    "                        \"content\": result[\"messages\"][-1].content,\n",
    "                        \"images\": [],\n",
    "                        \"videos\": []\n",
    "                    }\n",
    "\n",
    "                st.markdown(response[\"content\"])\n",
    "                \n",
    "                # ì´ë¯¸ì§€ í‘œì‹œ\n",
    "                if response.get(\"images\"):\n",
    "                    cols = st.columns(len(response[\"images\"]))\n",
    "                    for idx, img in enumerate(response[\"images\"]):\n",
    "                        with cols[idx]:\n",
    "                            st.image(img[\"url\"], caption=img[\"alt\"])\n",
    "                \n",
    "                # ë¹„ë””ì˜¤ í‘œì‹œ\n",
    "                if response.get(\"videos\"):\n",
    "                    for video in response[\"videos\"]:\n",
    "                        if video[\"type\"] == \"embed\":\n",
    "                            st.video(video[\"url\"])\n",
    "                        else:\n",
    "                            st.markdown(f\"[ë¹„ë””ì˜¤ ë³´ê¸°]({video['url']})\")\n",
    "\n",
    "                st.session_state.messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response[\"content\"],\n",
    "                    \"images\": response.get(\"images\", []),\n",
    "                    \"videos\": response.get(\"videos\", [])\n",
    "                })\n",
    "\n",
    "def extract_media_from_response(response: str) -> dict:\n",
    "    \"\"\"ìŠ¤í¬ë˜í•‘ ì‘ë‹µì—ì„œ ë¯¸ë””ì–´ ì •ë³´ ì¶”ì¶œ\"\"\"\n",
    "    images = []\n",
    "    videos = []\n",
    "    \n",
    "    # ì´ë¯¸ì§€ URL ì¶”ì¶œ\n",
    "    img_pattern = r'!\\[(.*?)\\]\\((.*?)\\)'\n",
    "    for alt, url in re.findall(img_pattern, response):\n",
    "        images.append({\"url\": url, \"alt\": alt})\n",
    "    \n",
    "    # ë¹„ë””ì˜¤ URL ì¶”ì¶œ\n",
    "    video_lines = [line for line in response.split('\\n') \n",
    "                  if 'ì„ë² ë“œ ë™ì˜ìƒ:' in line or 'ë¹„ë””ì˜¤ ë§í¬:' in line]\n",
    "    for line in video_lines:\n",
    "        url = line.split(': ')[1].strip()\n",
    "        videos.append({\n",
    "            \"url\": url,\n",
    "            \"type\": \"embed\" if 'ì„ë² ë“œ' in line else \"video\"\n",
    "        })\n",
    "    \n",
    "    return {\"images\": images, \"videos\": videos}\n",
    "\n",
    "def process_response(response: str, media_info: dict) -> dict:\n",
    "    \"\"\"ì‘ë‹µ ì²˜ë¦¬ ë° í¬ë§·íŒ…\"\"\"\n",
    "    return {\n",
    "        \"content\": response,\n",
    "        \"images\": media_info[\"images\"],\n",
    "        \"videos\": media_info[\"videos\"]\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_streamlit_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5e92a",
   "metadata": {},
   "source": [
    "streamlit run app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24517d59",
   "metadata": {},
   "source": [
    "ìƒíƒœ ê´€ë¦¬\n",
    "\n",
    "\n",
    "ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸íŠ¸\n",
    "\n",
    "ê·¸ë˜í”„ êµ¬ì¡°\n",
    "\n",
    "LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ì™€ ì²´í¬í¬ì¸íŠ¸ ë…¸ë“œë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.\n",
    "Gradio ì¸í„°í˜ì´ìŠ¤\n",
    "\n",
    "ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ì—¬ ì‚¬ìš©ìì™€ ìƒí˜¸ì‘ìš©í•©ë‹ˆë‹¤.\n",
    "ì‹¤ì œ êµ¬í˜„ ì‹œì—ëŠ” ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ì¶”ê°€ë¡œ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤:\n",
    "\n",
    "ì‹¤ì œ ì›¹ ìŠ¤í¬ë˜í•‘ ë¡œì§\n",
    "ìì„¸í•œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\n",
    "ì—ëŸ¬ ì²˜ë¦¬\n",
    "ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”\n",
    "ë³´ì•ˆ ê´€ë ¨ ê¸°ëŠ¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-uv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
